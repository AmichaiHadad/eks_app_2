# Cluster configuration
clusterName: eks-blizzard-us-east-1

# Global configuration
global:
  region: us-east-1

# Alertmanager webhook secret is expected to be managed externally
# Either created manually or via External Secrets
# The secret should be named 'alertmanager-slack-webhook' in the monitoring namespace
# with a key 'slack_webhook_url' containing the actual webhook URL
# DO NOT put webhook URLs in this file or templates

# Monitoring stack configuration
kube-prometheus-stack:
  # Global settings
  global:
    rbac:
      create: true
      
  # Add Elasticsearch as additional datasource to kube-prometheus-stack Grafana
  grafana:
    additionalDataSources:
      - name: Elasticsearch
        type: elasticsearch
        url: http://elasticsearch-master:9200
        access: proxy
        database: k8s-logs-*
        isDefault: false
        jsonData:
          timeField: "@timestamp"
          esVersion: 70
          logLevelField: level
          logMessageField: log
    
  # Prometheus Operator configuration
  prometheusOperator:
    enabled: true
    nodeSelector:
      node-role: monitoring
    tolerations:
      - key: monitoring
        value: "true"
        effect: NoSchedule
    admissionWebhooks:
      enabled: true
      patch:
        nodeSelector:
          node-role: monitoring
        tolerations:
          - key: monitoring
            value: "true"
            effect: NoSchedule
    createCustomResource: false
  
  # Prometheus configuration
  prometheus:
    enabled: true
    serviceAccount:
      create: true
    nodeSelector:
      node-role: monitoring
    tolerations:
      - key: monitoring
        value: "true"
        effect: NoSchedule
    prometheusSpec:
      replicas: 1
      retention: 15d
      resources:
        requests:
          cpu: 500m
          memory: 2Gi
        limits:
          cpu: 1000m
          memory: 4Gi
      storageSpec:
        volumeClaimTemplate:
          spec:
            storageClassName: gp2
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 50Gi
      # Additional scrape configurations for MySQL and Elasticsearch
      additionalScrapeConfigs:
        - job_name: mysql-exporter
          kubernetes_sd_configs:
            - role: endpoints
              namespaces:
                names:
                  - monitoring
          relabel_configs:
            - source_labels: [__meta_kubernetes_service_name]
              regex: prometheus-mysql-exporter
              action: keep
        - job_name: elasticsearch-exporter
          kubernetes_sd_configs:
            - role: endpoints
              namespaces:
                names:
                  - monitoring
          relabel_configs:
            - source_labels: [__meta_kubernetes_service_name]
              regex: prometheus-elasticsearch-exporter
              action: keep
        - job_name: keda-metrics-apiserver
          kubernetes_sd_configs:
            - role: endpoints
              namespaces:
                names:
                  - keda
          relabel_configs:
            - source_labels: [__meta_kubernetes_service_name]
              regex: keda-metrics-apiserver
              action: keep
      
      # Alerting rules
      ruleSelector: {}
      ruleNamespaceSelector: {}
      ruleSelectorNilUsesHelmValues: true
      serviceMonitorSelector: {}
      serviceMonitorNamespaceSelector: {}
      serviceMonitorSelectorNilUsesHelmValues: true
      podMonitorSelector: {}
      podMonitorNamespaceSelector: {}
      podMonitorSelectorNilUsesHelmValues: true
  
  # Alertmanager configuration
  alertmanager:
    enabled: true
    nodeSelector:
      node-role: monitoring
    tolerations:
      - key: monitoring
        value: "true"
        effect: NoSchedule
    config:
      global:
        resolve_timeout: 5m
      route:
        group_by: ['alertname', 'job']
        group_wait: 30s
        group_interval: 5m
        repeat_interval: 12h
        receiver: 'null'
        routes:
          - match:
              severity: critical
            receiver: 'null'
      receivers:
        - name: 'null'
      inhibit_rules:
        - source_match:
            severity: 'critical'
          target_match:
            severity: 'warning'
          equal: ['alertname', 'namespace']
    # Enable alertmanager config CRD
    alertmanagerConfigSelector:
      matchLabels: {}
    alertmanagerConfiguration:
      enabled: true
    alertmanagerSpec:
      storage:
        volumeClaimTemplate:
          spec:
            storageClassName: gp2
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 10Gi
  
  # Grafana configuration
  grafana:
    enabled: true
    nodeSelector:
      node-role: monitoring
    tolerations:
      - key: monitoring
        value: "true"
        effect: NoSchedule
    # Instead of using adminPassword directly, use an existing secret
    admin:
      existingSecret: "grafana-admin-credentials"
      userKey: "admin-user"
      passwordKey: "admin-password"
    persistence:
      enabled: true
      storageClassName: gp2
      size: 10Gi
    ingress:
      enabled: true
      ingressClassName: alb
      annotations:
        kubernetes.io/ingress.class: alb
        alb.ingress.kubernetes.io/scheme: internet-facing
        alb.ingress.kubernetes.io/target-type: ip
        alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}, {"HTTPS": 443}]'
        alb.ingress.kubernetes.io/ssl-redirect: "443"
        alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-east-1:163459217187:certificate/4ff90f30-64f8-40e1-b1b3-8f13d5fac876
        external-dns.alpha.kubernetes.io/hostname: grafana-{{ .Values.global.region }}.blizzard.co.il
        external-dns.alpha.kubernetes.io/ttl: "300"
    sidecar:
      dashboards:
        enabled: true
        searchNamespace: ALL
    # We are now managing datasources through kube-prometheus-stack.grafana.additionalDataSources
    # to avoid the duplicate datasource issue that causes Grafana to crash
    # datasources:
    #   datasources.yaml:
    #     apiVersion: 1
    #     datasources:
    #     - name: Prometheus
    #       type: prometheus
    #       url: http://prometheus-operated:9090
    #       access: proxy
    #       isDefault: true
    #     - name: Elasticsearch
    #       type: elasticsearch
    #       url: http://elasticsearch-master:9200
    #       access: proxy
    #       database: k8s-logs-*
    #       isDefault: false
    #       jsonData:
    #         timeField: "@timestamp"
    #         esVersion: 70
    #         logLevelField: level
    #         logMessageField: log
  
    # Dashboards configuration
    dashboardProviders:
      dashboardproviders.yaml:
        apiVersion: 1
        providers:
        - name: 'default'
          orgId: 1
          folder: ''
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/default
    
    # Disable pre-installed dashboards to avoid label issues
    # We'll use custom dashboards via ConfigMaps instead
    dashboards: {}
  
  # Kube state metrics
  kubeStateMetrics:
    enabled: true
    nodeSelector:
      node-role: monitoring
    tolerations:
      - key: monitoring
        value: "true"
        effect: NoSchedule
  
  # Node exporter
  nodeExporter:
    enabled: true
    tolerations:
      - operator: Exists
  
  # Thanos ruler for long-term query
  thanosRuler:
    enabled: false
  
  # Default values for Prometheus rules
  defaultRules:
    create: true
    appNamespace:
      rule:
        enabled: true
    rules:
      alertmanager: true
      etcd: true
      general: true
      k8s: true
      kubeApiserver: true
      kubePrometheusNodeAlerting: true
      kubePrometheusNodeRecording: true
      kubernetesAbsent: true
      kubernetesApps: true
      kubernetesResources: true
      kubernetesStorage: true
      kubernetesSystem: true
      kubeScheduler: true
      network: true
      node: true
      prometheus: true
      time: true

# MySQL exporter
prometheus-mysql-exporter:
  mysql:
    host: "mysql.data"
    port: 3306
    user: "prometheus"
    pass: "prometheus-exporter-password-placeholder"  # Will be overridden by External Secrets
    db: "app_db"
  
  nodeSelector:
    node-role: monitoring
  
  tolerations:
    - key: monitoring
      value: "true"
      effect: NoSchedule
  
  resources:
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 50m
      memory: 64Mi
  
  serviceMonitor:
    enabled: true

# Elasticsearch exporter
prometheus-elasticsearch-exporter:
  es:
    uri: http://elasticsearch-master:9200
    all: true
    indices: true
    indices_settings: true
    cluster_settings: true
    ssl:
      enabled: false
  
  nodeSelector:
    node-role: monitoring
  
  tolerations:
    - key: monitoring
      value: "true"
      effect: NoSchedule
  
  resources:
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 50m
      memory: 64Mi
  
  serviceMonitor:
    enabled: true